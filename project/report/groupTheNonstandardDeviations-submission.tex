\documentclass[10pt,conference,compsocconf]{IEEEtran}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage[group-separator = {\,}]{siunitx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[backend=biber,style=ieee]{biblatex}
\bibliography{groupTheNonstandardDeviations-literature}
\usepackage{hyperref}

\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}
\title{Computational Intelligence Laboratory Project: Collaborative Filtering}

\author{\IEEEauthorblockN{%
  Dina Zverinski\IEEEauthorrefmark{1},
  Jan Wilken D\"orrie\IEEEauthorrefmark{2} and
  \'Alvaro Marco A\~n\'o\IEEEauthorrefmark{3}}

\IEEEauthorblockA{%
  Group: TheNonstandardDeviations\\
  Department of Computer Science, ETH Zurich, Switzerland\\
  Email: \IEEEauthorrefmark{1}zdina@student.ethz.ch,
  \IEEEauthorrefmark{2}dojan@student.ethz.ch,
  \IEEEauthorrefmark{3}malvaro@student.ethz.ch}
}
\maketitle

\begin{abstract}
  % Short description of the whole paper, to help the reader decide whether to
  % read it.

  % The abstract should really be written last, along with the title of the
  % paper. The four points that should be covered:
  %   - State the problem.
  %   - Say why it is an interesting problem.
  %   - Say what your solution achieves.
  %   - Say what follows from your solution.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
% Describe your problem and state your contributions.

\section{Models and Methods}
\label{sec:models_and_methods}
% Describe your idea and how it was implemented to solve the problem. Survey the
% related work, giving credit where credit is due.

% The models and methods section should describe what was done to answer the
% research question, describe how it was done, justify the experimental design,
% and explain how the results were analyzed.

% The model refers to the underlying mathematical model or structure which you
% use to describe your problem, or that your solution is based on.  The methods
% on the other hand, are the algorithms used to solve the problem.  In some
% cases, the suggested method directly solves the problem, without having it
% stated in terms of an underlying model. Generally though it is a better
% practice to have the model figured out and stated clearly, rather than
% presenting a method without specifying the model. In this case, the method can
% be more easily evaluated in the task of fitting the given data to the
% underlying model.

% The methods part of this section, is not a step-by-step, directive, protocol
% as you might see in your lab manual, but detailed enough such that an
% interested reader can reproduce your work.

% The methods section of a research paper provides the information by which a
% study's validity is judged.  Therefore, it requires a clear and precise
% description of how an experiment was done, and the rationale for why specific
% experimental procedures were chosen.  It is usually helpful to structure the
% methods section by:
% - Layout the model you used to describe the problem or the solution.
% - Describing the algorithms used in the study, briefly including details such
%   as hyperparameter values (e.g. thresholds), and preprocessing steps (e.g.
%   normalizing the data to have mean value of zero).
% - Explaining how the materials were prepared, for example the images used and
%   their resolution.
% - Describing the research protocol, for example which examples were used for
%   estimating the parameters (training) and which were used for computing
%   performance.
% - Explaining how measurements were made and what calculations were performed.
%   Do not reproduce the full source code in the paper, but explain the key
%   steps.

In this section we describe what pre-existing algorithms we based our work on
and how we used blending in the end to combine the predictions of several
independent approaches. Furthermore we explain how we optimized our grade by
finding a good quality / CPU time trade-off.

\subsection{Preliminaries}
\label{sub:preliminaries}

Throughout this section we make frequent use of the following notation. We
denote by $\mathcal K$ the set of all existing ratings, i.e.\ $\mathcal K =
\{(u,i) \mid \text{user $u$ rated item $i$}\}$. Additionally we define $M$ and
$N$ to be the number of all users and items, respectively. In addition we define
the set of ratings for user's and item's, $R(u) = \{i \mid (u,i) \in \mathcal
K\}$ and $R(i) = \{u \mid (u,i) \in \mathcal K\}$. Furthermore we define the
true rating of an item $i$ by user $u$ as $r_{ui}$ and denote this rating's
approximation by $\hat{r}_{ui}$. Moreover we make frequent use of Greek letters,
where $\mu = \abs{\mathcal K}^{-1} \sum_{(u,i) \in \mathcal K} r_{ui}$ is the
global mean of all issued ratings, $\lambda$ denotes regularization constants
and $\gamma$ is the learning rate in an Stochastic Gradient Descent algorithm.

\subsection{Data Imputation and Baseline Estimators}
\label{sub:data_imputation_and_baseline_estimators}

Given that standard Collaborative Filtering algorithms such as K-Means and
Singular Value Decomposition (SVD) require dense instead of sparse matrices
different strategies exist to fill in the missing values. The most simple one is
to replace all missing values with a constant zero value. Accuracy can be
improved by considering other constants such as the global mean $\mu$ of all
existing entries.  More advanced techniques try to approximate a given missing
value for a user $u$ and item $i$ by taking a combination the global mean and
the specific user and item means $b_u$ and $b_i$~\cite{koren2008factorization,
  koren2009matrix, koren2010factor, koren2011advances}:

\begin{equation}
  b_{ui} = \mu + b_u + b_i.
\end{equation}

$b_u$ and $b_i$ can be directly computed from the data, however it is advisable
to introduce regularize terms to overcome over-fitting when only very few
ratings are available. This leads to the following equations, where $R(u)$ and
$R(i)$ denote the sets of all ratings by user $u$ and item $i$
respectively~\cite{koren2008factorization, koren2009matrix, koren2010factor,
  koren2011advances}:

\begin{equation}
  b_i = \frac{\sum_{u \in R(i)} (r_{ui} - \mu)}{\lambda_i + \abs{R(i)}}\qquad
  b_u = \frac{\sum_{i \in R(u)} (r_{ui} - \mu - b_i)}{\lambda_u + \abs{R(u)}}
\end{equation}

$\lambda_i$ and $\lambda_u$ are regularize parameters that should be found via
cross-validation. Finally it is also possible to estimate $b_u$ and $b_i$ by
solving the regularized least squares problem~\cite{koren2008factorization,
  koren2009matrix, koren2010factor, koren2011advances}

\begin{equation}
  \min_{b_*} \sum_{(u,i) \in \mathcal K} {(r_{ui} - \mu - b_u - b_i)}^2 +
  \lambda \left( \sum_u b_u^2 + \sum_i b_i^2 \right ).
\end{equation}

Possible solution approaches include Alternating Least Squares (ALS) or
Stochastic Gradient Descent (SGD). The baseline estimators are able to capture a
lot of signal present in the data so that it is advisable to make this
preprocessing step part of every more involved algorithm. For example, the RMSE
score for a simple SVD solution improved drastically when missing data was
imputed with optimized baseline estimators.

\subsection{Factorized Models}
\label{sub:factorized_models}

Motivated by the progress a simple SVD approach could make we investigated more
sophisticated approaches.

\subsubsection{Regularized SVD}
\label{ssub:regularized_svd}

Shortly after the Netflix Challenge started in 2006 Simon Funk proposed the idea
of a ``regularized SVD''~\cite{funk2006netflix}. In contrast to an ordinary
Singular Value Decomposition this approach does not rely on the imputation of
missing values, but only considers actual present ratings for training. The
algorithm tries to find two matrices $\mathbf P \in \mathbb R^{M \times K}$ and
$\mathbf Q \in \mathbb R^{N \times K}$ that accurately represent user-item
interactions. Both users and items get transformed to the same latent factor
space of a fixed dimension $K$ where their dot product is taken to measure their
compatibility. A rating $r_{ui}$ is then approximated by $\hat{r}_{ui} = b_{ui}
+ \mathbf q_i^{T}\mathbf p_u$. In order to avoid over-fitting to the data a
regularize term is added that penalizes large magnitudes of $\mathbf p_u$ and
$\mathbf q_i$. The associated least squares problem is the following:

\begin{equation}
  \min_{\mathbf q_*,\mathbf p_*} \sum_{(u,i) \in \mathcal K} {(r_{ui} - b_{ui} +
    \mathbf q_i^T \mathbf p_u)}^2 +
  \lambda (\norm{\mathbf q_i} + \norm{\mathbf p_u})
\end{equation}

This problem again can be solved using either ALS or SGD\@. Here we assume that
the biases have been estimated in a preprocessing step, however it is also
possible to learn them at the same time as $\mathbf P$ and $\mathbf Q$.

\subsubsection{SVD++}
\label{ssub:SVDpp}

Based on the work of Simon Funk, several improvements to regularized SVD were
suggested, that also consider the item-item relations present in the data.
Theses methods include ``NSVD''~\cite{paterek2007improving} and
``Asymmetric-SVD''~\cite{koren2008factorization} which both aim to model the
user vector $\mathbf p_u$ based on the items that given user rated. For example
in NSVD $\mathbf p_u$ is modeled via the sum $\left( \sum_{j \in R(u)} \mathbf
  x_j \right ) / \sqrt{\abs{R(u)}}$ where $\mathbf X \in \mathbb R^{N \times K}$
is a item dependent factor matrix learned from the data. The term
$\abs{R(u)}^{-1/2}$ serves as a user dependent normalization constant, which
stabilizes the sum's variance across the range of observed values of
$\abs{R(u)}$~\cite{koren2011advances}.

Building on top of these ideas two very similar models were developed, that
again include an explicit $\mathbf p_u$ vector for every user. These models are
dubbed ``MF-NSVD1''~\cite{takacs2008unified} and
``SVD++''~\cite{koren2008factorization, koren2011advances} respectively. In
SVD++ in addition to the user vector $\mathbf p_u$ the sum over all rated items
is kept, which leads to the following approximation formula:

\begin{equation}
  \hat{r}_{ui} = b_{ui} + \mathbf q_i^T \left( \mathbf p_u + \abs{R(u)}^{-1/2}
    \sum_{j \in R(u)} \mathbf x_j \right )
\end{equation}

Observing that the modified user term is completely independent of the current
item leads to a very quick learning and prediction algorithm that has a linear
time complexity with regard to the input size. Similar to previous models we
solved the regularized least squares problem using SGD.\@

\subsection{Neighborhood Models}
\label{sub:neighborhood_models}

Historically neighborhood models enjoyed a huge popularity in Collaborative
Filtering applications. These models include user-user and item-item based
models. In user-user models one tries to find similar users to a given user $u$
and use their existing ratings to recommend new items to $u$. Conversely, in
item-item based models one tries to find similar items to the items a given
user already rated, and recommend those. Usually the number of distinct items
are less than the number of distinct users which results in item-item models
being more efficient, because it is faster to compute similarities between all
items than it is to compute them between all users. In addition, item-item
models tend to provide a better user experience, because they offer an
explanation for their recommendations and users are usually more familiar with
their previous rated items than other users that are supposedly similar to them.

\subsubsection{A factorized User-User Model}
\label{ssub:a_factorized_user_user_model}

Given the fact that we already integrated an item-item model into the
regularized SVD to obtain SVD++ we shift our focus here to user-user models.
One possible approach to implement such a model is given with the following
formula~\cite{koren2010factor, koren2011advances}:

\begin{equation}
  \hat{r}_{ui} = b_{ui} + \abs{R(i)}^{-1/2} \sum_{v \in R(i)} (r_{vj} -
  b_{vj})w_{uv}
\end{equation}

Here $\mathbf W \in \mathbb R^{M \times M}$ is the similarity matrix between
users. These similarities are weighted by the difference between the real
rating and the baseline estimate. This is motivated by the fact that we want
to be able to adjust our actual estimate when either the difference to the
estimate or the weight between users is large. If either one is close to zero,
it means that either our baseline estimate is already very good or the current
users are not similar, so that we do not want to alter our current
estimate~\cite{koren2010factor, koren2011advances}.

The current model has time and space complexity both quadratic in the number of
users, which is prohibitively expensive when the number of users is large.
Assuming that the weights can be expressed as $w_{uv} = \mathbf p_u^T \mathbf
z_v$ for suitable choices of $\mathbf P, \mathbf Z \in \mathbb R^{M\times K}$
remedies this effect. Using this assumption, the rating estimate can be
expressed as the following~\cite{koren2010factor, koren2011advances}:

\begin{equation}
  \hat{r}_{ui} = b_{ui} + \abs{R(i)}^{-1/2} \mathbf p_u^T \left( \sum_{v \in
      R(i)} (r_{vj} - b_{vj}) \mathbf z_v \right )
\end{equation}

The term containing the sum does not depend on the current user $u$ and thus
can be precomputed, leading to linear time and space complexity.
% \subsection{Clustering}
% \label{sub:clustering}


\subsection{Blending of Results}
\label{sub:blending_of_results}


% \subsection{Optimizing for Speed}
% \label{sub:optimizing_for_speed}


\section{Results}
\label{sec:results}
% Show evidence to support your claims made in the introduction.

% Organize the results section based on the sequence of table and figures you
% include. Prepare the tables and figures as soon as all the data are analyzed
% and arrange them in the sequence that best presents your findings in a logical
% way. A good strategy is to note, on a draft of each table or figure, the one
% or two key results you want to address in the text portion of the results.
% The information from the figures is summarized in Table.

% When reporting computational or measurement results, always report the mean
% (average value) along with a measure of variability (standard deviation(s) or
% standard error of the mean).

\subsection{Dataset}
\label{sub:dataset}

The dataset which was used throughout the development of our algorithm was made
available on the project course webpage~\cite{lab2015collaborative}. It contains
the ratings of \num{10000} users on \num{1000} movies on a scale between $1$ and
$5$. Naturally, not every user rated every movie, in fact only \num{1388107} of
the \num{10000000} possible ratings were present leading to a data sparsity of
$86\%$.

\subsection{Evaluation of Results}
\label{sub:evaluation_of_results}

In order to train and test the developed algorithm the present ratings were
randomly split in two disjoint sets of equal size. The first set then formed
the training set, i.e.\ data that was trained on, and the second set was used
for testing the algorithm. The chosen metric is ``Root Mean Squared Error''
(RMSE) which is defined in the following way~\cite{koren2008factorization,
  koren2010factor, koren2011advances}:

\begin{equation}
  \sqrt{\sum_{(u,i) \in \mathrm{TestSet}}
    \frac{{(r_{ui} - \hat{r}_{ui})}^2}{\abs{\mathrm{TestSet}}}}
\end{equation}

Here $r_{ui}$ denotes the real rating while $\hat{r}_{ui}$ is the approximation
through the algorithm. With ratings restricted to the interval $[1,5]$ valid
RMSE values range between $0$ and $4$, with lower scores being better. In
addition, the raw CPU time was evaluated for the project grade. This favors
solutions that are efficient with regard to time.



\section{Discussion}
\label{sec:discussion}
% Discuss the strengths and weaknesses of your approach, based on the results.
% Point out the implications of your novel idea on the application concerned.

\section{Summary}
\label{sec:summary}
% Summarize your contributions in light of the new results.

\renewcommand*{\UrlFont}{\rmfamily}
\printbibliography%
\end{document}
