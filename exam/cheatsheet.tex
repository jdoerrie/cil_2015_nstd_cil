\documentclass[11pt,a4paper,technote]{IEEEtran}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{interval}
\usepackage[margin=1cm]{geometry}
\usepackage{bm}
\usepackage{fix-cm}
\newtheorem*{thm}{Theorem}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\intervalconfig{soft open fences}
\newcommand{\iver}[1]{\left[#1\right]}
\newcommand{\card}[1]{\left\lvert#1\right\rvert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inner}[2]{\langle#1,#2\rangle}
\newcommand{\matr}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\trns}[1]{#1^{\top}}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\likely}{\mathcal{L}}
\newcommand{\unif}[2]{\mathcal{U}\{#1,#2\}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\DeclareMathOperator{\len}{len}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\SVD}{SVD}
\newcommand{\from}{\colon}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}

\begin{document}
\title{CIL Cheat Sheet 2015}
\author{Jan Wilken D\"orrie}
\maketitle

\vspace{-2em}
% \section*{Linear Algebra Primer}
% \subsection*{Equivalent Conditions}
% For $\matr{A} \in \R^{M \times M}$ the following conditions
% are equivalent:
% % \begin{itemize}
%    $\matr{A}$ has an inverse $\matr{A}^{-1}$;
%    $\mathrm{rank}(\matr{A}) = M$;
%    $\mathrm{range}(\matr{A}) = \R^M$;
%    $\mathrm{null}(\matr{A}) = \{\vect{0}\}$;
%    $0$ is not an eigenvalue of $\matr{A}$;
%    $0$ is not a singular value of $\matr{A}$.
% % \end{itemize}


\section*{Norms}
\subsection*{Vector norms}
A \emph{norm} is a function $\norm{\cdot} : V \to \R$ quantifying the
size of a vector. It must satisfy
\begin{enumerate}
  \item Positive scalability: $\norm{a \cdot \vect{x}} = \abs{a} \cdot
    \norm{\vect{x}}$ for $a \in \R$
  \item Triangle inequality: $\norm{\vect{x} + \vect{y}} \leq
    \norm{\vect{x}} + \norm{\vect{y}}$, $\vect{x}, \vect{y} \in V$.
  \item Seperability: $\norm{\vect{x}} = 0$ implies $\vect{x} = 0$.
\end{enumerate}

\begin{itemize}
  \item Most common are \emph{$p$-norms}:
    \(
      \norm{\vect{x}}_p := {\left(\sum_{i=1}^n \abs{x_i}^p \right)}^{1/p}
    \)
  \item Special case is \emph{Euclidean norm}:
    \( \norm{\vect{x}}_2 := \sqrt{\sum_{i=1}^n x_i^2} \)
  \item The ``\emph{$0$-norm}'' is \( \norm{\vect{x}}_0 := \card{\{x_i \mid x_i
        \neq 0\}} \)
\end{itemize}

\subsection*{Matrix norms}
We can also define norms on matrices, satisfying the properties described
above. $\matr{A} \in \R^{M \times N}$:
\begin{itemize}
  \item \emph{Frobenius}:
    \(
      \norm{\matr{A}}_F := \sqrt{\sum_{ij} a_{ij}^2}
      = \sqrt{\sum_{i=1}^{\min(M,N)} \sigma_i^2}
    \)

  \item \emph{$p$-norms for matrices}:
    \(
      \norm{\matr{A}}_p := \sup\{\norm{\matr{A}\vect{x}}_p/\norm{\vect{x}}_p\}
    \)
  \item \emph{Euclidean}:
    \(
      \norm{\matr{A}}_2 := \sup\{\norm{\matr{A}\vect{x}}_2/\norm{\vect{x}}_2\}
      = \sigma_{\max}
    \)
  \item \emph{Nuclear norm}:
    \(
      \norm{\matr{A}}_{*} := \sum_{i=1}^{\min(M,N)} \sigma_i
    \)

\end{itemize}

\section*{Statistics}
\subsection*{Kullback-Leibler Divergence}
\begin{itemize}
  \item Divergence between discrete probability distributions $P$ and $Q$:
    \(
      D_{\mathrm{KL}}(P \| Q) = \sum_{\omega \in \Omega} P(\omega)
      \log\left(\frac{P(\omega)}{Q(\omega)}\right).
    \)
  \item Properties of the Kullback-Leibler Divergence
    \begin{itemize}
      \item $D_{\mathrm{KL}}(P \| Q) \geq 0$.
      \item $D_{\mathrm{KL}}(P \| Q) = 0$ if and only if $P$ and $Q$ are
        identical.
      \item $D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)$!
      \item caution: the KL-Divergence is not symmetric, therefore it is not a
        metric/distance!
    \end{itemize}
\end{itemize}

\section*{Dimension Reduction}
\subsection*{Principal Component Analysis (PCA)}
Orthogonal linear projection of high dimensional data onto low
dimensional subspace. Objectives:
\begin{enumerate}
  \item Minimize error $\norm{\vect{x} - \tilde{\vect{x}}}_2$ of
    point $\vect{x}$ and its approximation $\tilde{\vect{x}}$.
  \item Preserve information: maximize variance.
\end{enumerate}
Both objectives are shown to be formally equivalent.
\subsubsection*{Statistics of Projected Data}
\begin{itemize}
  \item Mean of the data: sample mean $\bar{\vect{x}}$
  \item Covariance of the data:
    \[
      \matr{\Sigma} = \frac{1}{N} \sum_{n=1}^N (\vect{x}_n -
      \bar{\vect{x}}) \trns{(\vect{x}_n - \bar{\vect{x}})}
    \]
\end{itemize}
\subsubsection*{Solution: Eigenvalue Decomposition}
The eigenvalue decomposition of the covariance matrix
$\matr{\Sigma} = \matr{U}\matr{\Lambda}\trns{\matr{U}}$ contains
all relevant information.
\begin{itemize}
  \item For $K \leq D$ dimensional projection space: Choose $K$
    eigenvectors $\{\vect{u}_1, \ldots, \vect{u}_K\}$ with largest
    associated eigenvalues $\{\lambda_1, \ldots, \lambda_K\}$.
\end{itemize}

\subsection*{Singular Value Decomposition}
\begin{thm}[Eckart-Young]
  Let $\matr{A}$ be a matrix of rank $R$, if we wish to
  approximate $\matr{A}$ using a matrix of a lower rank $K$
  then, $\tilde{\matr{A}} Ìƒ= \sum_{k=1}^K d_k \vect{u}_k
  \trns{\vect{v}_k}$ is the closest matrix in the Frobenius norm.
  (Assumes ordering of singular values $d_k \geq d_{k+1}$)
\end{thm}

\section*{Clustering}
\subsection*{K-Means}
\subsubsection*{Motivation}
\begin{itemize}
  \item Given: set of data points $\vect{x}_1, \ldots, \vect{x}_N \in \R^D$
  \item Goal: find \emph{meaningful partition} of the data
    \begin{itemize}
      \item i.e.\ a labeling of each data point with a unique label
        \[
          \pi: \{1,\ldots,N\} \to \{1,\ldots,K\} \text{ or }
          \pi: \R^D \to \{1,\ldots,K\}
        \]
      \item note: numbering of clusters is arbitrary
      \item $k$-th cluster recovered by $\pi^{-1}(k) \subseteq \{1,\ldots,N\}$
        or $\subseteq \R^D$
    \end{itemize}
\end{itemize}

\subsubsection*{Vector Quantization}
\begin{itemize}
  \item Partition of the space $\R^D$
  \item Clusters represented by \emph{centroids} $\vect{u}_k \in \R^D$
  \item Mapping induced via nearest centroid rule
    \[ \pi(\vect{x}) = \argmin_{k=1,\ldots,K} \norm{\vect{u}_k - \vect{x}}_2 \]
\end{itemize}

\subsubsection*{Objective Function for $K$-Means}
\begin{itemize}
  \item Useful notation: represent $\pi$ via indicator matrix $\matr{Z}$:
    \[ z_{kn} := \iver{\pi(\vect{x}_n) = k} \]
  \item $K$-means Objective function
    \[
      J(\matr{U},\matr{Z}) = \sum_{n=1}^N \sum_{k=1}^K z_{kn} \norm{\vect{x}_n
        - \vect{u}_k}_2^2 = \norm{\matr{X} - \matr{U}\matr{Z}}_F^2,
    \]
    where $\matr{X} = [\vect{x}_1,\ldots,\vect{x}_N] \in \R^{D \times N}$
    and   $\matr{U} = [\vect{u}_1,\ldots,\vect{u}_K] \in \R^{D \times K}$
\end{itemize}

\subsubsection*{$K$-means Algorithm: Optimal Assignment}
\begin{itemize}
  \item Compute optimal assignment $\matr{Z}$, given centroids $\matr{U}$
    \begin{itemize}
      % \item each data point contributes to exactly one term in outer sum
      \item minimize each column of $\matr{Z}$ separately
        \[
          \vect{z}_{\bullet n}^* = \argmin_{z_{1n},\ldots,z_{Kn}} \sum_{k=1}^K
          z_{kn} \norm{\vect{x}_n - \vect{u}_k}_2^2
        \]
      \item optimum is attained by mapping to the closest centroid
        \[
          z_{kn}^*(\matr{U}) = \iver{k = \argmin_l \norm{\vect{x} - \vect{u}_l}_2}
        \]
    \end{itemize}
\end{itemize}

\subsubsection*{$K$-means Algorithm: Optimal Assignment}
\begin{itemize}
  \item Compute optimal choice of $\matr{U}$, given assignments $\matr{Z}$
    \begin{itemize}
      \item continuous variables: compute gradient and set to zero (necessary
        optimality condition)
      \item look at (partial) gradient for every centroid $\vect{u}_k$
        \[
          \nabla_{\vect{u}_k} J(\matr{U},\matr{Z}) = \sum_{n=1}^N z_{kn}
          \nabla_{\vect{u}_k} \norm{\vect{x}_n - \vect{u}_k}_2^2 = -2
          \sum_{n=1}^N z_{kn} (\vect{x}_n - \vect{u}_k)
        \]
      \item setting gradient to zero
        \[
          \nabla_{\matr{U}} J(\matr{U},\matr{Z}) \stackrel{!}{=} 0 \implies
          \vect{u}_k^*(\matr{Z}) = \frac{\sum_{n=1}^N z_{kn} \vect{x}_n}
          {\sum_{n=1}^N z_{kn}} %\text{, if } \sum_{n=1}^N z_{kn} > 0
        \]
    \end{itemize}
\end{itemize}

\subsubsection*{$K$-means Algorithm: Analysis}
\begin{itemize}
  \item Computational cost of each iteration is $O(KND)$
  \item $K$-means convergence is guaranteed
  \item $K$-means optimizes a non-convex objective. Hence we are not guaranteed
    to find the global optimum.
  \item Finds a local optimum $(\matr{U},\matr{Z})$ in the following sense
    \begin{itemize}
      \item for each $\matr{Z}'$ with $\frac{1}{2}\norm{\matr{Z}-\matr{Z}'}_0 = 1$
        (differs in one assignment)
      \item $J(\matr{U}^*(\matr{Z}'), \matr{Z}') \geq J(\matr{U},\matr{Z})$
      \item may gain by changing assignments of $\geq 2$ points
    \end{itemize}
  \item $K$-means algorithm can be used to compress data
    \begin{itemize}
      \item with information loss, if $K < N$
      \item store only the centroids and the assignments
    \end{itemize}
\end{itemize}
% Alternate between two steps until convergence:
% \begin{enumerate}
%   \item Update assignments $z_{k,n}$ of data points to centroids:
%     \[
%       z_{k,n} = \begin{cases}
%         1, &\text{if $k = \argmin_j \norm{\vect{x}_n - \vect{u}_j}_2$} \\
%         0, &\text{else}
%       \end{cases}
%     \]
%   \item Update centroid positions:
%     \[
%       \vect{u}_k = \frac{\sum_n z_{k,n} \vect{x}_n}{\sum_n z_{k,n}}
%     \]
% \end{enumerate}

\subsection*{Mixture Models}
\subsubsection*{GMM}
\(
  p_{\theta}(\vect{x}) = \sum_{k} \pi_k \normal(\vect{x}; \vect{\mu}_k,
  \matr{\Sigma}_k),
\)
$\vect{\pi}\geq\vect{0}$, $\norm{\vect{\pi}}_1 = 1$.

\subsubsection*{Complete Data Distribution}
\begin{itemize}
  \item Explicitly introduce latent variables in the generative model
  \item Assignment variable (for a generic data point) $z_k \in \{0,1\}$,
    $\sum_{k=1}^K z_k = 1$
  \item We have that $\Pr(z_k = 1) = \pi_k$ or $p(\vect{z}) = \prod_{k=1}^K
    \pi_k^{z_k}$
  \item Joint distribution over $(\vect{x}, \vect{z})$ (\emph{complete data}
    distribution) $p(\vect{x}, \vect{z}) = \prod_{k=1}^K {\left[ \pi_k
        \normal(\vect{x} \mid \vect{\mu}_k, \matr{\Sigma}_k)\right]}^{z_k}$
\end{itemize}

\subsubsection*{Posterior Assignments}
\emph{Posterior probabilities} for assignments
\[
  \Pr(z_k = 1 \mid \vect{x}) = \frac{\pi_k \normal(\vect{x} \mid \vect{\mu}_k,
    \matr{\Sigma}_k)}{\sum_{l=1}^K \pi_l \normal(\vect{x} \mid \vect{\mu}_l,
    \matr{\Sigma}_l)}
\]

\subsubsection*{Lower Bounding the Log-Likelihood}
\begin{itemize}
  \item Expectation Maximization
    \begin{itemize}
      \item maximize a lower bound on the log-likelihood
      \item systematic way of deriving a family of bounds
      \item based on complete data distribution
    \end{itemize}
  \item Specifically:
    \begin{align*}
      \ln p_{\theta}(\vect{x}) &= \ln \sum_{\vect{z}} p_{\theta}(\vect{x},
      \vect{z}) = \ln \sum_{k=1}^K p(\vect{x}, \theta_k) \pi_k \\
      &= \ln \sum_{k=1}^K q_k \frac{p(\vect{x}; \theta_k) \pi_k}{q_k} \\
      &\geq \sum_{k=1}^K q_k \left[\ln p(\vect{x},\theta_k)+\ln\pi_k-\ln q_k \right]
    \end{align*}
    \begin{itemize}
      \item follows from Jensen's inequality (concavity of logarithm)
      \item can be done for the contribution of each data point (additive)
    \end{itemize}
\end{itemize}

\subsubsection*{Mixture Model: Expectation Step}
\[
  q_k = \frac{\pi_k\; p(\vect{x}; \theta_k)}{\sum_{l=1}^K \pi_l\;p(\vect{x}, \theta_l)}
  = \Pr(z_k = 1 \mid \vect{x})
\]

\subsubsection*{Mixture Model: Maximization Step}
\( \pi_k^* = \frac{1}{N} \sum_{n=1}^N q_{kn} \),
\( \vect{\mu}_k^* = \frac{\sum_{n=1}^N q_{kn}\vect{x}_n}{\sum_{n=1}^N q_{kn}} \)
and
\(
  \matr{\Sigma}_k^* = \frac{\sum_{n=1}^N q_{kn}(\vect{x}_n -
    \vect{\mu}_k^*)\trns{(\vect{x}_n - \vect{\mu}_k^*)}}{\sum_{n=1}^N q_{kn}}
\)

\subsubsection*{AIC and BIC}
\begin{itemize}
  \item Trade-off: achieve balance between data fit --- measured by likelihood
    $p(\matr{X}\mid\theta)$ --- and complexity. Complexity can be measured by
    the number of free parameters $\kappa(\cdot)$.
  \item Different Heuristics for choosing $K$
    \begin{itemize}
      \item \emph{Akaike Information Criterion} (AIC)
        \[
          \mathrm{AIC}(\theta\mid\matr{X}) = -\ln p_{\theta}(\matr{X}) +
          \kappa(\theta)
        \]
      \item \emph{Bayesian Information Criterion} (BIC)
        \[
          \mathrm{BIC}(\theta\mid\matr{X}) = -\ln p_{\theta}(\matr{X}) +
          \frac{1}{2} \kappa(\theta) \ln N
        \]
    \end{itemize}
  \item Generally speaking, the BIC criterion penalizes complexity more than
    the AIC criterion.
\end{itemize}

\subsection*{Non-Negative Matrix Factorization}
\subsubsection*{Non-Negative Matrix Factorization}
\begin{itemize}
  \item \emph{Document-term matrix} $\matr{X} \in \R_{\geq 0}^{D \times N}$
    storing the word counts for each document:
    \[ \matr{X} = \vect{x}_1, \vect{x}_2, \ldots, \vect{x}_N \]

    $x_{dn}$: Frequency of the $d$-th word in the $n$-th document.

  \item \emph{Non-negative matrix factorization} (NMF) of $\matr{X}$:
    \[ \matr{X} \approx \matr{U}\matr{Z} \]
    \begin{itemize}
      \item with $\matr{U} \in \R_{\geq 0}^{D \times K}$ and $\matr{Z} \in
        \R_{\geq 0}^{K \times N}$
        \begin{itemize}
          \item $N$: number of documents
          \item $D$: vocabulary size
          \item $K$: number of dimensions (design choice)
          \item data reduction: $(D+N)K \ll DN$
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection*{pLSI --- Generative Model}
\begin{itemize}
  \item For a given \emph{document} sample \emph{$\len(\mathrm{document})$}
    words by a two-stage procedure:
    \begin{itemize}
      \item sample a topic according to $P(\mathrm{topic}\mid\mathrm{document})$
      \item sample a word according to $P(\mathrm{word}\mid\mathrm{topic})$
    \end{itemize}
  \item Key assumption: \emph{conditional independence} of word and document
    given topic
  \item Conditional distribution of a word, given a document:
    \[
      P(\mathrm{word}\mid\mathrm{document}) = \sum_{k=1}^K P(\mathrm{word}\mid
      \mathrm{topic}_k) P(\mathrm{topic}_k \mid \mathrm{document})
    \]
  \item Side note: how to sample a ``new'' document? Can use fully generative
    model of LDA.\@
\end{itemize}

\subsubsection*{pLSI --- Matrix Factorization View}
\begin{itemize}
  \item \emph{Normalize} the elements of $\matr{X}$ so that they correspond to
    relative frequencies:
    \[
      T := \sum_{d=1}^D \sum_{n=1}^N x_{dn}, \qquad x_{dn}\gets\frac{x_{dn}}{T}
    \]
  \item \emph{Matrix Factorization}
    \begin{itemize}
      \item pLSI can be understood as a matrix factorization of the form
        $\matr{X}\approx\matr{U}\matr{Z}$, with $\matr{U}\in\R_{\geq 0}^{D
          \times K}$, and $\matr{Z}\in\R_{\geq 0}^{K \times N}$
      \item where additionally we have the constraints:
        \begin{itemize}
          \item $\sum_{d=1}^D u_{dk} = 1 (\forall k)$, identify $u_{dk} \equiv
            P(\mathrm{word}_d \mid \mathrm{topic}_k)$
          \item $\sum_{k,n} z_{kn} = 1$, identify $z_{kn} \equiv
            P(\mathrm{topic}_k \mid \mathrm{document}_n) P(\mathrm{document}_n)$
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection*{pLSI --- Parameter Estimation}
\begin{itemize}
  \item Goal: maximize the likelihood of the data under the model
  \item Data: the relative frequencies $\matr{X}$
  \item Probabilistic model:
    \(
      P(\mathrm{word}_d, \mathrm{document}_n) = \sum_{k=1}^K P(\mathrm{word}_d \mid
      \mathrm{topic}_k) P(\mathrm{topic}_k \mid \mathrm{document}_n)
      = {(\matr{U}\matr{Z})}_{dn}
    \)
  \item \emph{Log likelihood}:
    \(
      \log\likely(\matr{U}, \matr{Z}; \matr{X}) = \log P(\matr{X}; \matr{U},
      \matr{Z}) = \sum_{d=1}^D\sum_{n=1}^N x_{dn} \log \sum_{k=1}^K
      u_{dk}z_{kn}
    \)
\end{itemize}

\subsubsection*{EM for pLSI --- Variational Likelihood}
\begin{itemize}
  \item Follow similar recipe as for Gaussian Mixture Model
  \item Reindex the observations in a per token manner with $t = 1, \ldots, T$
    \begin{itemize}
      \item pairs of word/documents indexes $(d_t, n_t)$
      \item note that $\sum_{t=1}^T f(d_t,n_t) = \sum_{d=1}^D\sum_{n=1}^N
        x_{dn}f(d,n)$ for arbitrary functions $f$
    \end{itemize}
  \item \emph{Variational Likelihood}
    \begin{align*}
      &\log P(\matr{X};\matr{U},\matr{Z}) \\
      &= \sum_{t=1}^T \log{(\matr{U} \matr{Z})}_{d_{t}n_{t}} = \sum_{t=1}^T
      \log\left[\sum_{k=1}^K u_{d_{t}k} z_{kn_{t}}\right] \\
      &\geq \sum_{t=1}^T \max_{q\in\set{S}_K} \sum_{k=1}^K q_k [\log
      u_{d_{t}k} + \log z_{kn_{t}} - \log q_k ]
    \end{align*}
    \begin{itemize}
      \item $\set{S}_K := \left\{x\in\R^K\mid x\geq 0, \sum_{k=1}^K x_k =
          1\right\}$ (probability simplex)
    \end{itemize}
\end{itemize}

\subsubsection*{EM for pLSI --- Derivation of E-step}
\begin{itemize}
  \item Compute the argmin in the variational bound
    \[
      q_t^* = \argmax_{q\in\set{S}_K}\sum_{k=1}^K q_k [\log u_{d_{t}k} +
      \log z_{kn_{t}} + \log q_k]
    \]
  \item Form Lagrangian and differentiate
    \begin{align*}
      &\frac{\partial}{\partial q_k} \{ q_k [\log u_{d_{t}k} + \log z_{kn_{t}}
      - \log q_k - \lambda_t^*] \} \stackrel{!}{=} 0 \\
      \implies &q_{tk}^* \propto u_{d_{t}k}z_{kn_{t}}\text{, i.e. }q_{tk}^* =
      \frac{u_{d_{t}k}z_{kn_{t}}}{\sum_{l=1}^K u_{d_{t}l}z_{l n_{t}}}
    \end{align*}
  \item $q_{tk}^* =$ posterior probability that $t$-th token (i.e.\ word with
    index $d_t$ in document with index $n_t$) has been generated from topic $k$
\end{itemize}

\subsubsection*{EM for pLSI --- Derivation of M-step}
\begin{itemize}
  \item Differentiate lower bound with plugged in optimal choices for $q_t^*$
    $(t = 1,\ldots,T)$
  \item \emph{M}-step solution for $\matr{U}$ and $\matr{Z}$
    \[
      u_{dk}^* = \frac{\sum_{t:d_t=d} q_{tk}^*}{\sum_{t=1}^T q_{tk}^*}
      % = \frac{\text{\# tokens $t$ equal to $d$, weighted by $q_{tk}^*$}}
      % {\text{\# tokens $t$ of any word, weighted by $q_{tk}^*$}}
      \qquad
      z_{kn}^* = \frac{\sum_{t:n_t=n} q_{tk}^*}{T}
      % = \frac{\text{\# tokens $t$ equal to $d$, weighted by $q_{tk}^*$}}
      % {\text{\# tokens $t$ of any word, weighted by $q_{tk}^*$}}
    \]

\end{itemize}

\section*{Sparse Coding}
\subsection*{Optimization}
\subsubsection*{Coordinate Descent}
\emph{Idea}: Update one coordinate at a time, while keeping others fixed.
\begin{itemize}
  \item Algorithm:
    \begin{itemize}
      \item initialize $\vect{x}^{(0)} \in \R^D$
      \item for $t = 0,\ldots,\mathrm{maxIter}$
        \begin{itemize}
          \item $d \gets \unif{1}{D}$
          \item $u^* \gets \argmin_{u\in\R} f\left(x_1^{(t)}, \ldots,
              x_{d-1}^{(t)}, u, x_{d+1}^{(t)}, \ldots, x_D^{(t)}\right)$
          \item $x_d^{(t+1)}\gets u^*,\quad x_{d'}^{(t+1)}\gets x_{d'}^{(t)}$
            for $d' \neq d$
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection*{Gradient Descent Method}
\begin{itemize}
  \item Algorithm:
    \begin{itemize}
      \item initialize $\vect{x}^{(0)} \in \R^D$
      \item for $t = 0,\ldots,\mathrm{maxIter}$
        \begin{itemize}
          \item $\vect{x}^{(t+1)} \gets \vect{x}^{(t)} - \gamma\nabla
            f\left(\vect{x}^{(t)}\right)$
        \end{itemize}
    \end{itemize}
  \item simple to implement
  \item good scalability and robustness
  \item \emph{stepsize} $\gamma$ usually decreasing with
    $\gamma\approx\frac{1}{t}$
\end{itemize}

\subsubsection*{Stochastic Gradient Descent}
\begin{itemize}
  \item Optimization Problem Structure: minimize $f(\vect{x}) = \frac{1}{N}
    \sum_{n=1}^N f_n(\vect{x})$ with $\vect{x} \in \R^D$
\end{itemize}
\begin{itemize}
  \item Algorithm:
    \begin{itemize}
      \item initialize $\vect{x}^{(0)} \in \R^D$
      \item for $t = 0,\ldots,\mathrm{maxIter}$
        \begin{itemize}
          \item $n\gets\unif{1}{N}$
          \item $\vect{x}^{(t+1)} \gets \vect{x}^{(t)} - \gamma\nabla
            f_n\left(\vect{x}^{(t)}\right)$
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection*{Duality for Constrained Optimization}
\begin{itemize}
  \item Constrained Problem Formulation (Standard Form): minimize $f(\vect{x})$
    subject to $g_i(\vect{x}) \leq 0$, $i = 1, \ldots, m$, $h_i(\vect{x}) = 0$,
    $i = 1,\ldots,p$
  \item Unconstrained Problem: minimze $f(\vect{x}) + \sum_{i=1}^m
    I_{-}(g_i(\vect{x})) + \sum_{i=1}^p I_{0}(h_i(\vect{x}))$. $I_{-}$ and $I_{0}$
    are ``brickwall'' indicator functions.
\end{itemize}

\subsubsection*{Dual Problem}
\begin{itemize}
  \item Lagrangian: $L(\vect{x},\vect{\lambda},\vect{\nu}) := f(\vect{x}) +
    \sum_{i=1}^m \lambda_i g_i(\vect{x}) + \sum_{i=1}^p \nu_i h_i(\vect{x})$
  \item Lagrange dual function: $d(\vect{\lambda},\vect{\nu}) := \inf_{\vect{x}}
    L(\vect{x},\vect{\lambda},\vect{\nu})$
  \item Lagrange \emph{dual problem}: maximize $d(\vect{\lambda},\vect{\nu})$
    subject to $\vect{\lambda} \geq \vect{0}$.

    It is always a lower bound on the primal value $f(\vect{x})$ of any
    feasible $\vect{x}$ and thus a lower bound on the unknown solution
    value $f(\vect{x}^*)$ of the primal problem.

    \emph{Strong Duality}: If the primal optimization problem is convex
    and under some additional conditions, the solution value of the dual
    problem is \emph{equal} to the solution value $f(\vect{x}^*)$ of the
    primal problem.
    % \end{itemize}
\end{itemize}

\subsubsection*{Convexity}
\begin{itemize}
  \item Convex Set: A set $\set{Q}$ is convex if for any $\vect{x},\vect{y} \in
    \set{Q}$ and any $\theta\in\interval{0}{1}$, we have $\theta\vect{x} +
    (1-\theta) \vect{y} \in \set{Q}$.
  \item Convex Function: A function $f\from\R^D\to\R$ is convex if $\dom f$ is
    a convex set and if for all $\vect{x},\vect{y}\in\dom f$, and $\theta\in
    \interval{0}{1}$ we have $f(\theta\vect{x} + (1-\theta)\vect{y}) \leq \theta
    f(\vect{x}) + (1-\theta)f(\vect{y})$.
  \item Convex Optimization: Convex Optimization Problems are of the form $\min
    f(\vect{x})$ s.t.\ $\vect{x}\in \set{Q}$ where both $f$ is a convex
    function and $\set{Q}$ is a convex set (note: $\R^D$ is convex). In Convex
    Optimization Problems every local minimum is a \emph{global minimum}.
\end{itemize}

\vspace{-1em}
\subsection*{Sparse Coding}
\subsubsection*{Properties of Haar Wavelets}
\begin{itemize}
  \item Mother wavelet: $\psi(t) = \iver{t\in\interval[open
      right]{0}{\frac{1}{2}}} - \iver{t\in\interval[open right]{\frac{1}{2}}{1}}$
    \item Haar function: $\psi_{n,k}(t) = 2^{n/2}\psi(2^{n}t-k)$, $n,k\in\Z$
    \item $\psi_{n,k}(t)$ non-zero on $I_{n,k} = \interval[open right]{k2^{-n}}
      {(k+1)2^{-n}}$
    \item Integral $0$: $\int_{\R}\psi_{n,k}(t)\diff t = 0$
    \item Norm $1$: $\norm{\psi_{n,k}}_{L^2(\R)}^2 = \int_{\R}
      {\psi_{n,k}(t)}^2\diff t = 1$
    \item Orthogonal: $\int_{\R}\psi_{n_1,k_1}(t) \psi_{n_2,k_2}(t)
      \diff t = \delta_{n_1,n_2}\delta_{k_1,k_2}$
    \item $\implies$ Haar system is orthonormal basis in $L^2(\R)$
\end{itemize}
Any continuous real function on $\interval{0}{1}$ can be approximated uniformly
on $\interval{0}{1}$ by linear combinations of the constant function $\vect{1},
\psi(t),\psi(2t),\psi(4t),\dotsc,\psi(2^n t),\dotsc$ and their shifted functions.

\subsubsection*{Discrete cosine transform (DCT)}
\begin{itemize}
  \item 1D DCT:\@ $z_k = \sum_{n=0}^{N-1}x_n\cos[\frac{\pi}{N}(n+\frac{1}{2})k]$
  \item 2D DCT:\@ $ z_{k_1,k_2} = \sum_{n_1=0}^{N_1-1} \sum_{n_2=0}^{N_2-1}
    x_{n_1,n_2} \cos [\frac{\pi}{N_1} (n_1+\frac{1}{2}) k_1 ] \cos
    [\frac{\pi}{N_2}(n_2+\frac{1}{2}) k_2]$
\end{itemize}

\subsubsection*{Compressive Sensing}
\emph{Main idea}: acquire the set $\vect{y}$ of $M$ linear combinations
of the initial signal instead of the signal itself and then
reconstruct the initial signal from these measurements. $\vect{y} =
\matr{W}\vect{x} = \matr{W}\matr{U}\vect{z} =: \matr{\Theta}\vect{z}$,
with $\matr{\Theta} = \matr{W}\matr{U} \in \R^{M \times D}$.
Surprisingly given any orthonormal basis $\matr{U}$ we can obtain a
stable reconstruction for any $K$-sparse, compressible signal. Two
conditions: $w_{ij} \stackrel{\text{i.i.d.}}{\sim} \normal(0,\frac{1}{D})$ and
$M \geq cK\log(\frac{D}{K})$, $c\in\R$. For $M \ll D$ ill-posed, hence
$\vect{z}^*\in \argmin_{\vect{z}} \norm{\vect{z}}_0$, s.t.\ $\vect{y} =
\matr{\Theta}\vect{\vect{z}}$. NP-hard, approximate with \emph{Matching Pursuit}
or do convex relaxation with $\norm{\vect{z}}_1$.

\subsubsection*{Coding via orthogonal transforms} Given orig.\ signal $\vect{x}$
and orthogonal matrix $\matr{U}$ compute change of basis $\vect{z} =
\trns{\matr{U}} \vect{x}$. Truncate ``small'' values, giving $\hat{\vect{z}}$.
Compute inverse transform $\hat{\vect{x}} = \matr{U}\hat{\vect{z}}$.

\emph{Measure performance}: error $\norm{\vect{x}-\hat{\vect{x}}}$ and sparsity
$\norm{\vect{z}}_0$.

\emph{Dictionary choice}: Fourier Dictionary is good for ``sine like'' signals,
Wavelet Dictionary is good for localized signals.

\subsection*{Overcomplete Dictionaries}
In contract to othogonal bases there are more atoms than dimensions ($L > D$).
Coding algorithm chooses best representation (subset of atoms), but this is
mathematically involved due to non-orthogonality (no closed form reconstruction).

\subsubsection*{Coherence}
Increasing the Overcompleteness factor $\frac L D$ potentially increases the
sparsity of the coding, but also increases the linear dependency between atoms.
Coherence is a measurement for this: $m(\matr{U}) = \max_{i,j:i\neq j}
\abs{\trns{\vect{u}_i} \vect{u}_j}$. $m(\matr{U}) = 0$ for an orthogonal basis
$\matr{B}$, $m([\matr{B}\vect{u}]) \geq \frac{1}{\sqrt{D}}$ if atom $\vect{u}$
is added to orthogonal $\matr{B}$.

\subsubsection*{Signal Coding}
$\matr{U}\in\R^{D \times L}$ is overcomplete, so finding $\vect{z}$ such that
$\vect{x} = \matr{U}\vect{z}$ is ill-posed, more unknowns than equations. Need
to add sparsity constraint: $\vect{z}^*\in\argmin_{\vect{z}} \norm{\vect{z}}_0$
s.t.\ $\vect{x} = \matr{U}\vect{z}$. Problem is NP-hard, can be brute-forced for
small instances, needs Matching Pursuit else.

\emph{Noisy Observations}: Signal might be corrupted, $\vect{x} = \matr{U}
\vect{z} + \vect{n}$ with $n_d \sim \normal(0,\sigma^2)$. Solve either
$\vect{z}^* \in \argmin_{\vect{z}}\norm{\vect{z}}_0$ s.t.\ $\norm{\vect{x} -
  \matr{U}\vect{z}}_2^2 < D\sigma^2$ or $\vect{z}^* \in \argmin_{\vect{z}}
\norm{\vect{x} - \matr{U} \vect{z}}_2$ s.t.\ $\norm{\vect{z}}_0 \leq K$.

\subsubsection*{Matching Pursuit (MP) Algorithm} Greedy algorithm that starts
with zero vector $\vect{z} = \vect{0}$ and residual $\vect{r}^0 = \vect{x}$. At
each iteration $t$ selects atom with maximal absolute correlation to residual
$d^* \gets \argmax_d \abs{\trns{\vect{u}_d} \vect{r}^{(t)}}$ and updates
vectors $z_{d^*} \gets z_{d^*} + \trns{\vect{u}_{d^*}} \vect{r}^{(t)}$,
$\vect{r}^{(t+1)} \gets \vect{r}^{(t)} - (\trns{\vect{u}_{d^*}} \vect{r}^{(t)})
\vect{u}_{d^*}$. Stops when $\norm{\vect{z}}_0 = K$. MP is an approximation,
but recovers exact coding when $K < \frac{1}{2}(1 + \frac{1}{m(\matr{U})})$.

\subsubsection*{Sparse Coding for Inpainting}
Define diagonal masking matrix $\matr{M}$, $m_{d,d} = \iver{\text{pixel $d$ is
    known}}$, sparse coding of known parts in overcomplete dictionary
$\matr{U}$: $\vect{z}^* \in \argmin_{\vect{z}} \norm{\vect{z}}_0$ s.t.\
$\norm{\matr{M}(\vect{x} - \matr{U}\vect{z})}_2 < \sigma$. Image reconstruction
using mask: $\hat{\vect{x}} = \matr{M}\vect{x} + (\matr{I} - \matr{M})
\matr{U}\vect{z}^*$.

\subsection*{Dictionary Learning}
When learning the dictionary we adapt a dictionary to signal
characteristics in the data, for which we have to solve a matrix
factorization problem $\matr{X} = \matr{U}\matr{Z}$ with sparsity
constraint on $\matr{Z}$ and atom norm constraint on $\matr{U}$.
$(\matr{U}^*, \matr{Z}^*) \in \argmin_{\matr{U},\matr{Z}} \norm{\matr{X} -
  \matr{U}\matr{Z}}_F^2$, objective not jointly convex over $\matr{U}$ and
$\matr{Z}$ but convex in either of them when the other one is fixed.

\emph{Iterative greedy minimization}:
\begin{enumerate}
  \item Coding step: $\matr{Z}^{(t+1)}\in\argmin_{\matr{Z}}\norm{\matr{X} -
      \matr{U}^{(t)}\matr{Z}}_F^2$, subject to $\matr{Z}$ being sparse and
    $\matr{U}$ being fixed.
  \item Dict.\ update: $\matr{U}^{(t+1)}\in\argmin_{\matr{U}} \norm{\matr{X} -
      \matr{U}\matr{Z}^{(t+1)}}_F^2$, subject to $\norm{\vect{u}_l}_2 = 1$ for
    all $l$ and $\matr{Z}$ being fixed.
\end{enumerate}
Coding step can be done column-wise via \emph{Matching Pursuit} and dictionary
update via \emph{K-SVD} algorithm involving a power iteration to approximate SVD
solution. Dictionary learning can also be used for Speech Enhancement by learning
the Speech and Noise dictionaries and then setting the noise coefficients to zero.

\section*{Robust PCA}
Goal: Find a low rank representation of a matrix $\matr{X}$, which is corrupted
by a sparse perturbation or sparse structured noise.
Additive decomposition problem: $\min_{\matr{L},\matr{S}} \rank(\matr{L}) +
\lambda \norm{\matr{S}}_0$ s.t.\ $\matr{L}+\matr{S} = \matr{X}$. This
problem is non-convex and thus hard to solve, convex relaxation:
$\min_{\matr{L},\matr{S}} \norm{\matr{L}}_* + \lambda \norm{\matr{S}}_1$ s.t.\
$\matr{L}+\matr{S} = \matr{X}$. This is \emph{not} the same problem, but
achieves the same solution under broad conditions.

\subsubsection*{Alternating Direction Method of Multiplies (ADMM)}
$\min_{\vect{x}_1,\vect{x}_2} f_1(\vect{x}_1) + f_2(\vect{x}_2)$ s.t.\
$\matr{A}_1 \vect{x}_1 + \matr{A}_2\vect{x}_2 = \vect{b}$ with $f_1$, $f_2$
convex. \emph{Augmented Lagrangian}: $L_{\rho}(\vect{x}_1,\vect{x}_2,\vect{\nu})
= f_1(\vect{x}_1) + f_2(\vect{x}_2) + \trns{\vect{\nu}}(\matr{A}_1\vect{x}_1 +
\matr{A}_2 \vect{x}_2 - \vect{b}) + \frac{\rho}{2}\norm{\matr{A}_1\vect{x}_1 +
  \matr{A}_2\vect{x}_2 - \vect{b}}_2^2$, punishes violations of the constraints
even more. Update steps: $\vect{x}_1^{(t+1)} := \argmin_{\vect{x}_1}
L_{\rho}(\vect{x}_1, \vect{x}_2^{(t)}, \vect{\nu}^{(t)})$, $\vect{x}_2^{(t+1)}
:= \argmin_{\vect{x}_2} L_{\rho}(\vect{x}_1^{(t+1)}, \vect{x}_2,
\vect{\nu}^{(t)})$, $\vect{\nu}^{(t+1)} := \vect{\nu}^{(t)} + \rho(\matr{A}_1
\vect{x}_1^{(t+1)} + \matr{A}_2\vect{x}_2^{(t+1)} - \vect{b})$.

\subsubsection*{ADMM for RPCA}
Here $f_1(\vect{x}_1) = \norm{\matr{L}}_*$ and $f_2(\vect{x}_2) = \lambda
\norm{\matr{S}}_1$, hence $L_{\rho}(\matr{L},\matr{S},\matr{N}) =
\norm{\matr{L}}_* + \lambda\norm{\matr{S}}_1 + \inner{\matr{N}}{(\matr{L} +
  \matr{S} - \matr{X})} + \frac{\rho}{2}\norm{\matr{L} + \matr{S} -
  \matr{X}}_F^2$. Update sequence:
$\matr{L}^{(t+1)}:=\argmin_{\matr{L}}L_{\rho}(\matr{L},\matr{S}^{(t)},\matr{N}^{(t)})$,
$\matr{S}^{(t+1)}:=\argmin_{\matr{S}}L_{\rho}(\matr{L}^{(t+1)},\matr{S},\matr{N}^{(t)})$,
$\matr{N}^{(t+1)}:=\rho(\matr{L}^{(t+1)} + \matr{S}^{(t+1)} - \matr{X})$.
Solving explicitly:
$\argmin_{\matr{L}}L_{\rho}(\matr{L},\matr{S},\matr{N}) = \mathcal{D}_{\rho^{-1}}
(\matr{X} - \matr{S} - \rho^{-1}\matr{N})$,
$\argmin_{\matr{S}}L_{\rho}(\matr{L},\matr{S},\matr{N}) = \mathcal{S}_{\rho^{-1}}
(\matr{X} - \matr{L} - \rho^{-1}\matr{N})$, where $\mathcal{S}_{\tau}(x) =
\sgn(x)\max(\abs{x}-\tau, 0)$, $\mathcal{S}_{\tau}(\matr{X})$ applies
$\mathcal{S}_{\tau}$ to all $x_{ij}$, $\mathcal{D}_{\tau}(\matr{X}) = \matr{U}
\mathcal{S}_{\tau}(\matr{\Sigma})\trns{\matr{V}}$, where $\SVD(\matr{X}) =
\matr{U}\matr{\Sigma}\trns{\matr{V}}$.

\subsubsection*{Identifiability}
The solutions to the convex relaxation solve the original problem when the Coherence
condition is fulfilled (principal components must not be sparse (spiky)) and
both the rank of $\matr{L}_0$ and the number of non-zero entries of $\matr{Z}_0$
is not too large.

RPCA can be used for collaborative filtering when relaxing the constraints to
only consider known observations.

\end{document}
