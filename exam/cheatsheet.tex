\documentclass[11pt,a4paper,technote]{IEEEtran}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[margin=1cm]{geometry}
\usepackage{bm}
\newtheorem*{thm}{Theorem}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\newcommand{\iver}[1]{\left[#1\right]}
\newcommand{\card}[1]{\left\lvert#1\right\rvert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\matr}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\trns}[1]{#1^{\top}}
\newcommand{\normal}{\mathcal{N}}
\DeclareMathOperator{\len}{len}
\DeclareMathOperator{\dom}{dom}
\newcommand{\from}{\colon}
\newcommand{\R}{\mathbb{R}}

\begin{document}
\title{CIL Cheat Sheet 2015}
\author{Jan Wilken D\"orrie}
\maketitle

\vspace{-2em}
\section*{Linear Algebra Primer}
\subsection*{Equivalent Conditions}
For $\matr{A} \in \R^{M \times M}$ the following conditions
are equivalent:
% \begin{itemize}
   $\matr{A}$ has an inverse $\matr{A}^{-1}$;
   $\mathrm{rank}(\matr{A}) = M$;
   $\mathrm{range}(\matr{A}) = \R^M$;
   $\mathrm{null}(\matr{A}) = \{\vect{0}\}$;
   $0$ is not an eigenvalue of $\matr{A}$;
   $0$ is not a singular value of $\matr{A}$.
% \end{itemize}


\section*{Norms}
\subsection*{Vector norms}
A \emph{norm} is a function $\norm{\cdot} : V \to \R$ quantifying the
size of a vector. It must satisfy
\begin{enumerate}
  \item Positive scalability: $\norm{a \cdot \vect{x}} = \abs{a} \cdot
    \norm{\vect{x}}$ for $a \in \R$
  \item Triangle inequality: $\norm{\vect{x} + \vect{y}} \leq
    \norm{\vect{x}} + \norm{\vect{y}}$, $\vect{x}, \vect{y} \in V$.
  \item Seperability: $\norm{\vect{x}} = 0$ implies $\vect{x} = 0$.
\end{enumerate}

\begin{itemize}
  \item Most common are \emph{$p$-norms}:
    \(
      \norm{\vect{x}}_p := {\left(\sum_{i=1}^n \abs{x_i}^p \right)}^{1/p}
    \)
  \item Special case is \emph{Euclidean norm}:
    \( \norm{\vect{x}}_2 := \sqrt{\sum_{i=1}^n x_i^2} \)
  \item The ``\emph{$0$-norm}'' is \( \norm{\vect{x}}_0 := \card{\{x_i \mid x_i
        \neq 0\}} \)
\end{itemize}

\subsection*{Matrix norms}
We can also define norms on matrices, satisfying the properties described
above. $\matr{A} \in \R^{M \times N}$:
\begin{itemize}
  \item \emph{Frobenius}:
    \(
      \norm{\matr{A}}_F := \sqrt{\sum_{ij} a_{ij}^2}
      = \sqrt{\sum_{i=1}^{\min(M,N)} \sigma_i^2}
    \)

  \item \emph{$p$-norms for matrices}:
    \(
      \norm{\matr{A}}_p := \sup\{\norm{\matr{A}\vect{x}}_p/\norm{\vect{x}}_p\}
    \)
  \item \emph{Euclidean}:
    \(
      \norm{\matr{A}}_2 := \sup\{\norm{\matr{A}\vect{x}}_2/\norm{\vect{x}}_2\}
      = \sigma_{\max}
    \)
  \item \emph{Nuclear norm}:
    \(
      \norm{\matr{A}}_{*} := \sum_{i=1}^{\min(M,N)} \sigma_i
    \)

\end{itemize}

\section*{Statistics}
\subsection*{Kullback-Leibler Divergence}
\begin{itemize}
  \item Divergence between discrete probability distributions $P$ and $Q$:
    \(
      D_{\mathrm{KL}}(P \| Q) = \sum_{\omega \in \Omega} P(\omega)
      \log\left(\frac{P(\omega)}{Q(\omega)}\right).
    \)
  \item Properties of the Kullback-Leibler Divergence
    \begin{itemize}
      \item $D_{\mathrm{KL}}(P \| Q) \geq 0$.
      \item $D_{\mathrm{KL}}(P \| Q) = 0$ if and only if $P$ and $Q$ are
        identical.
      \item $D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)$!
      \item caution: the KL-Divergence is not symmetric, therefore it is not a
        metric/distance!
    \end{itemize}
\end{itemize}

\section*{Dimension Reduction}
\subsection*{Principal Component Analysis (PCA)}
Orthogonal linear projection of high dimensional data onto low
dimensional subspace. Objectives:
\begin{enumerate}
  \item Minimize error $\norm{\vect{x} - \tilde{\vect{x}}}_2$ of
    point $\vect{x}$ and its approximation $\tilde{\vect{x}}$.
  \item Preserve information: maximize variance.
\end{enumerate}
Both objectives are shown to be formally equivalent.
\subsubsection*{Statistics of Projected Data}
\begin{itemize}
  \item Mean of the data: sample mean $\bar{\vect{x}}$
  \item Covariance of the data:
    \[
      \matr{\Sigma} = \frac{1}{N} \sum_{n=1}^N (\vect{x}_n -
      \bar{\vect{x}}) \trns{(\vect{x}_n - \bar{\vect{x}})}
    \]
\end{itemize}
\subsubsection*{Solution: Eigenvalue Decomposition}
The eigenvalue decomposition of the covariance matrix
$\matr{\Sigma} = \matr{U}\matr{\Lambda}\trns{\matr{U}}$ contains
all relevant information.
\begin{itemize}
  \item For $K \leq D$ dimensional projection space: Choose $K$
    eigenvectors $\{\vect{u}_1, \ldots, \vect{u}_K\}$ with largest
    associated eigenvalues $\{\lambda_1, \ldots, \lambda_K\}$.
\end{itemize}

\subsection*{Singular Value Decomposition}
\begin{thm}[Eckart-Young]
  Let $\matr{A}$ be a matrix of rank $R$, if we wish to
  approximate $\matr{A}$ using a matrix of a lower rank $K$
  then, $\tilde{\matr{A}} Ìƒ= \sum_{k=1}^K d_k \vect{u}_k
  \trns{\vect{v}_k}$ is the closest matrix in the Frobenius norm.
  (Assumes ordering of singular values $d_k \geq d_{k+1}$)
\end{thm}

\section*{Clustering}
\subsection*{K-Means}
\subsubsection*{Motivation}
\begin{itemize}
  \item Given: set of data points $\vect{x}_1, \ldots, \vect{x}_N \in \R^D$
  \item Goal: find \emph{meaningful partition} of the data
    \begin{itemize}
      \item i.e.\ a labeling of each data point with a unique label
        \[
          \pi: \{1,\ldots,N\} \to \{1,\ldots,K\} \text{ or }
          \pi: \R^D \to \{1,\ldots,K\}
        \]
      \item note: numbering of clusters is arbitrary
      \item $k$-th cluster recovered by $\pi^{-1}(k) \subseteq \{1,\ldots,N\}$
        or $\subseteq \R^D$
    \end{itemize}
\end{itemize}

\subsubsection*{Vector Quantization}
\begin{itemize}
  \item Partition of the space $\R^D$
  \item Clusters represented by \emph{centroids} $\vect{u}_k \in \R^D$
  \item Mapping induced via nearest centroid rule
    \[ \pi(\vect{x}) = \argmin_{k=1,\ldots,K} \norm{\vect{u}_k - \vect{x}}_2 \]
\end{itemize}

\subsubsection*{Objective Function for $K$-Means}
\begin{itemize}
  \item Useful notation: represent $\pi$ via indicator matrix $\matr{Z}$:
    \[ z_{kn} := \iver{\pi(\vect{x}_n) = k} \]
  \item $K$-means Objective function
    \[
      J(\matr{U},\matr{Z}) = \sum_{n=1}^N \sum_{k=1}^K z_{kn} \norm{\vect{x}_n
        - \vect{u}_k}_2^2 = \norm{\matr{X} - \matr{U}\matr{Z}}_F^2,
    \]
    where $\matr{X} = [\vect{x}_1,\ldots,\vect{x}_N] \in \R^{D \times N}$
    and   $\matr{U} = [\vect{u}_1,\ldots,\vect{u}_K] \in \R^{D \times K}$
\end{itemize}

\subsubsection*{$K$-means Algorithm: Optimal Assignment}
\begin{itemize}
  \item Compute optimal assignment $\matr{Z}$, given centroids $\matr{U}$
    \begin{itemize}
      % \item each data point contributes to exactly one term in outer sum
      \item minimize each column of $\matr{Z}$ separately
        \[
          \vect{z}_{\bullet n}^* = \argmin_{z_{1n},\ldots,z_{Kn}} \sum_{k=1}^K
          z_{kn} \norm{\vect{x}_n - \vect{u}_k}_2^2
        \]
      \item optimum is attained by mapping to the closest centroid
        \[
          z_{kn}^*(\matr{U}) = \iver{k = \argmin_l \norm{\vect{x} - \vect{u}_l}_2}
        \]
    \end{itemize}
\end{itemize}

\subsubsection*{$K$-means Algorithm: Optimal Assignment}
\begin{itemize}
  \item Compute optimal choice of $\matr{U}$, given assignments $\matr{Z}$
    \begin{itemize}
      \item continuous variables: compute gradient and set to zero (necessary
        optimality condition)
      \item look at (partial) gradient for every centroid $\vect{u}_k$
        \[
          \nabla_{\vect{u}_k} J(\matr{U},\matr{Z}) = \sum_{n=1}^N z_{kn}
          \nabla_{\vect{u}_k} \norm{\vect{x}_n - \vect{u}_k}_2^2 = -2
          \sum_{n=1}^N z_{kn} (\vect{x}_n - \vect{u}_k)
        \]
      \item setting gradient to zero
        \[
          \nabla_{\matr{U}} J(\matr{U},\matr{Z}) \stackrel{!}{=} 0 \implies
          \vect{u}_k^*(\matr{Z}) = \frac{\sum_{n=1}^N z_{kn} \vect{x}_n}
          {\sum_{n=1}^N z_{kn}} %\text{, if } \sum_{n=1}^N z_{kn} > 0
        \]
    \end{itemize}
\end{itemize}

\subsubsection*{$K$-means Algorithm: Analysis}
\begin{itemize}
  \item Computational cost of each iteration is $O(KND)$
  \item $K$-means convergence is guaranteed
  \item $K$-means optimizes a non-convex objective. Hence we are not guaranteed
    to find the global optimum.
  \item Finds a local optimum $(\matr{U},\matr{Z})$ in the following sense
    \begin{itemize}
      \item for each $\matr{Z}'$ with $\frac{1}{2}\norm{\matr{Z}-\matr{Z}'}_0 = 1$
        (differs in one assignment)
      \item $J(\matr{U}^*(\matr{Z}'), \matr{Z}') \geq J(\matr{U},\matr{Z})$
      \item may gain by changing assignments of $\geq 2$ points
    \end{itemize}
  \item $K$-means algorithm can be used to compress data
    \begin{itemize}
      \item with information loss, if $K < N$
      \item store only the centroids and the assignments
    \end{itemize}
\end{itemize}
% Alternate between two steps until convergence:
% \begin{enumerate}
%   \item Update assignments $z_{k,n}$ of data points to centroids:
%     \[
%       z_{k,n} = \begin{cases}
%         1, &\text{if $k = \argmin_j \norm{\vect{x}_n - \vect{u}_j}_2$} \\
%         0, &\text{else}
%       \end{cases}
%     \]
%   \item Update centroid positions:
%     \[
%       \vect{u}_k = \frac{\sum_n z_{k,n} \vect{x}_n}{\sum_n z_{k,n}}
%     \]
% \end{enumerate}

\subsection*{Mixture Models}
\subsubsection*{Gaussian Mixture Model (GMM)}
\[
  p_{\theta}(\vect{x}) = \sum_{k=1}^K \pi_k \normal(\vect{x} \mid \vect{\mu}_k,
  \matr{\Sigma}_k),
\]
where $\pi_k \geq 0$, $\sum_{k=1}^K \pi_k = 1$.

\subsubsection*{Complete Data Distribution}
\begin{itemize}
  \item Explicitly introduce latent variables in the generative model
  \item Assignment variable (for a generic data point) $z_k \in \{0,1\}$,
    $\sum_{k=1}^K z_k = 1$
  \item We have that $\Pr(z_k = 1) = \pi_k$ or $p(\vect{z}) = \prod_{k=1}^K
    \pi_k^{z_k}$
  \item Joint distribution over $(\vect{x}, \vect{z})$ (\emph{complete data}
    distribution) $p(\vect{x}, \vect{z}) = \prod_{k=1}^K {\left[ \pi_k
        \normal(\vect{x} \mid \vect{\mu}_k, \matr{\Sigma}_k)\right]}^{z_k}$
\end{itemize}

\subsubsection*{Posterior Assignments}
\emph{Posterior probabilities} for assignments
\[
  \Pr(z_k = 1 \mid \vect{x}) = \frac{\pi_k \normal(\vect{x} \mid \vect{\mu}_k,
    \matr{\Sigma}_k)}{\sum_{l=1}^K \pi_l \normal(\vect{x} \mid \vect{\mu}_l,
    \matr{\Sigma}_l)}
\]

\subsubsection*{Lower Bounding the Log-Likelihood}
\begin{itemize}
  \item Expectation Maximization
    \begin{itemize}
      \item maximize a lower bound on the log-likelihood
      \item systematic way of deriving a family of bounds
      \item based on complete data distribution
    \end{itemize}
  \item Specifically:
    \begin{align*}
      \ln p_{\theta}(\vect{x}) &= \ln \sum_{\vect{z}} p_{\theta}(\vect{x},
      \vect{z}) = \ln \sum_{k=1}^K p(\vect{x}, \theta_k) \pi_k \\
      &= \ln \sum_{k=1}^K q_k \frac{p(\vect{x}; \theta_k) \pi_k}{q_k} \\
      &\geq \sum_{k=1}^K q_k \left[\ln p(\vect{x},\theta_k)+\ln\pi_k-\ln q_k \right]
    \end{align*}
    \begin{itemize}
      \item follows from Jensen's inequality (concavity of logarithm)
      \item can be done for the contribution of each data point (additive)
    \end{itemize}
\end{itemize}

\subsubsection*{Mixture Model: Expectation Step}
\[
  q_k = \frac{\pi_k\; p(\vect{x}; \theta_k)}{\sum_{l=1}^K \pi_l\;p(\vect{x}, \theta_l)}
  = \Pr(z_k = 1 \mid \vect{x})
\]

\subsubsection*{Mixture Model: Maximization Step}
\[ \pi_k^* = \frac{1}{N} \sum_{n=1}^N q_{kn} \]
\[ \vect{\mu}_k^* = \frac{\sum_{n=1}^N q_{kn}\vect{x}_n}{\sum_{n=1}^N q_{kn}} \]
\[
  \matr{\Sigma}_k^* = \frac{\sum_{n=1}^N q_{kn}(\vect{x}_n -
    \vect{\mu}_k^*)\trns{(\vect{x}_n - \vect{\mu}_k^*)}}{\sum_{n=1}^N q_{kn}}
\]

\subsubsection*{AIC and BIC}
\begin{itemize}
  \item Trade-off: achieve balance between data fit --- measured by likelihood
    $p(\matr{X}\mid\theta)$ --- and complexity. Complexity can be measured by
    the number of free parameters $\kappa(\cdot)$.
  \item Different Heuristics for choosing $K$
    \begin{itemize}
      \item \emph{Akaike Information Criterion} (AIC)
        \[
          \mathrm{AIC}(\theta\mid\matr{X}) = -\ln p_{\theta}(\matr{X}) +
          \kappa(\theta)
        \]
      \item \emph{Bayesian Information Criterion} (BIC)
        \[
          \mathrm{BIC}(\theta\mid\matr{X}) = -\ln p_{\theta}(\matr{X}) +
          \frac{1}{2} \kappa(\theta) \ln N
        \]
    \end{itemize}
  \item Generally speaking, the BIC criterion penalizes complexity more than
    the AIC criterion.
\end{itemize}

\subsection*{Non-Negative Matrix Factorization}
\subsubsection*{Non-Negative Matrix Factorization}
\begin{itemize}
  \item \emph{Document-term matrix} $\matr{X} \in \R_{\geq 0}^{D \times N}$
    storing the word counts for each document:
    \[ \matr{X} = \vect{x}_1, \vect{x}_2, \ldots, \vect{x}_N \]

    $x_{dn}$: Frequency of the $d$-th word in the $n$-th document.

  \item \emph{Non-negative matrix factorization} (NMF) of $\matr{X}$:
    \[ \matr{X} \approx \matr{U}\matr{Z} \]
    \begin{itemize}
      \item with $\matr{U} \in \R_{\geq 0}^{D \times K}$ and $\matr{Z} \in
        \R_{\geq 0}^{K \times N}$
        \begin{itemize}
          \item $N$: number of documents
          \item $D$: vocabulary size
          \item $K$: number of dimensions (design choice)
          \item data reduction: $(D+N)K \ll DN$
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection*{pLSI --- Generative Model}
\begin{itemize}
  \item For a given \emph{document} sample \emph{$\len(\mathrm{document})$}
    words by a two-stage procedure:
    \begin{itemize}
      \item sample a topic according to $P(\mathrm{topic}\mid\mathrm{document})$
      \item sample a word according to $P(\mathrm{word}\mid\mathrm{topic})$
    \end{itemize}
  \item Key assumption: \emph{conditional independence} of word and document
    given topic
  \item Conditional distribution of a word, given a document:
    \[
      P(\mathrm{word}\mid\mathrm{document}) = \sum_{k=1}^K P(\mathrm{word}\mid
      \mathrm{topic}_k) P(\mathrm{topic}_k \mid \mathrm{document})
    \]
  \item Side note: how to sample a ``new'' document? Can use fully generative
    model of LDA.\@
\end{itemize}

\subsubsection*{pLSI --- Matrix Factorization View}
\begin{itemize}
  \item \emph{Normalize} the elements of $\matr{X}$ so that they correspond to
    relative frequencies:
    \[
      T := \sum_{d=1}^D \sum_{n=1}^N x_{dn}, \qquad x_{dn}\gets\frac{x_{dn}}{T}
    \]
  \item \emph{Matrix Factorization}
    \begin{itemize}
      \item pLSI can be understood as a matrix factorization of the form
        $\matr{X}\approx\matr{U}\matr{Z}$, with $\matr{U}\in\R_{\geq 0}^{D
          \times K}$, and $\matr{Z}\in\R_{\geq 0}^{K \times N}$
      \item where additionally we have the constraints:
        \begin{itemize}
          \item $\sum_{d=1}^D u_{dk} = 1 (\forall k)$, identify $u_{dk} \equiv
            P(\mathrm{word}_d \mid \mathrm{topic}_k)$
          \item $\sum_{k,n} z_{kn} = 1$, identify $z_{kn} \equiv
            P(\mathrm{topic}_k \mid \mathrm{document}_n) P(\mathrm{document}_n)$
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection*{pLSI --- Parameter Estimation}
\begin{itemize}
  \item Goal: maximize the likelihood of the data under the model
  \item Data: the relative frequencies $\matr{X}$
  \item Probabilistic model:
    \(
      P(\mathrm{word}_d, \mathrm{document}_n) = \sum_{k=1}^K P(\mathrm{word}_d \mid
      \mathrm{topic}_k) P(\mathrm{topic}_k \mid \mathrm{document}_n)
      = {(\matr{U}\matr{Z})}_{dn}
    \)
  \item \emph{Log likelihood}:
    \(
      \log \mathcal{L}(\matr{U}, \matr{Z}; \matr{X}) = \log P(\matr{X}; \matr{U}, \matr{Z})
      = \sum_{d=1}^D\sum_{n=1}^N x_{dn} \log \sum_{k=1}^K u_{dk}z_{kn}
    \)
\end{itemize}

\subsubsection*{EM for pLSI --- Variational Likelihood}
\begin{itemize}
  \item Follow similar recipe as for Gaussian Mixture Model
  \item Reindex the observations in a per token manner with $t = 1, \ldots, T$
    \begin{itemize}
      \item pairs of word/documents indexes $(d_t, n_t)$
      \item note that $\sum_{t=1}^T f(d_t,n_t) = \sum_{d=1}^D\sum_{n=1}^N
        x_{dn}f(d,n)$ for arbitrary functions $f$
    \end{itemize}
  \item \emph{Variational Likelihood}
    \begin{align*}
      &\log P(\matr{X};\matr{U},\matr{Z}) \\
      &= \sum_{t=1}^T \log{(\matr{U} \matr{Z})}_{d_{t}n_{t}} = \sum_{t=1}^T
      \log\left[\sum_{k=1}^K u_{d_{t}k} z_{kn_{t}}\right] \\
      &\geq \sum_{t=1}^T \max_{q\in\mathcal{S}_K} \sum_{k=1}^K q_k [\log
      u_{d_{t}k} + \log z_{kn_{t}} - \log q_k ]
    \end{align*}
    \begin{itemize}
      \item $\mathcal{S}_K := \left\{x\in\R^K\mid x\geq 0, \sum_{k=1}^K x_k =
          1\right\}$ (probability simplex)
    \end{itemize}
\end{itemize}

\subsubsection*{EM for pLSI --- Derivation of E-step}
\begin{itemize}
  \item Compute the argmin in the variational bound
    \[
      q_t^* = \argmax_{q\in\mathcal{S}_K}\sum_{k=1}^K q_k [\log u_{d_{t}k} +
      \log z_{kn_{t}} + \log q_k]
    \]
  \item Form Lagrangian and differentiate
    \begin{align*}
      &\frac{\partial}{\partial q_k} \{ q_k [\log u_{d_{t}k} + \log z_{kn_{t}}
      - \log q_k - \lambda_t^*] \} \stackrel{!}{=} 0 \\
      \implies &q_{tk}^* \propto u_{d_{t}k}z_{kn_{t}}\text{, i.e. }q_{tk}^* =
      \frac{u_{d_{t}k}z_{kn_{t}}}{\sum_{l=1}^K u_{d_{t}l}z_{l n_{t}}}
    \end{align*}
  \item $q_{tk}^* =$ posterior probability that $t$-th token (i.e.\ word with
    index $d_t$ in document with index $n_t$) has been generated from topic $k$
\end{itemize}

\subsubsection*{EM for pLSI --- Derivation of M-step}
\begin{itemize}
  \item Differentiate lower bound with plugged in optimal choices for $q_t^*$
    $(t = 1,\ldots,T)$
  \item \emph{M}-step solution for $\matr{U}$ and $\matr{Z}$
    \[
      u_{dk}^* = \frac{\sum_{t:d_t=d} q_{tk}^*}{\sum_{t=1}^T q_{tk}^*}
      % = \frac{\text{\# tokens $t$ equal to $d$, weighted by $q_{tk}^*$}}
      % {\text{\# tokens $t$ of any word, weighted by $q_{tk}^*$}}
      \qquad
      z_{kn}^* = \frac{\sum_{t:n_t=n} q_{tk}^*}{T}
      % = \frac{\text{\# tokens $t$ equal to $d$, weighted by $q_{tk}^*$}}
      % {\text{\# tokens $t$ of any word, weighted by $q_{tk}^*$}}
    \]

\end{itemize}

\section*{Sparse Coding}
\subsection*{Optimization}
\subsubsection*{Coordinate Descent}
\emph{Idea}: Update one coordinate at a time, while keeping others fixed.
\begin{itemize}
  \item Algorithm:
    \begin{itemize}
      \item initialize $\vect{x}^{(0)} \in \R^D$
      \item for $t = 0,\ldots,\mathrm{maxIter}$
        \begin{itemize}
          \item $d \gets \mathcal{U}\{1,D\}$
          \item $u^* \gets \argmin_{u\in\R} f\left(x_1^{(t)}, \ldots,
              x_{d-1}^{(t)}, u, x_{d+1}^{(t)}, \ldots, x_D^{(t)}\right)$
          \item $x_d^{(t+1)}\gets u^*,\quad x_{d'}^{(t+1)}\gets x_{d'}^{(t)}$
            for $d' \neq d$
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection*{Gradient Descent Method}
\begin{itemize}
  \item Algorithm:
    \begin{itemize}
      \item initialize $\vect{x}^{(0)} \in \R^D$
      \item for $t = 0,\ldots,\mathrm{maxIter}$
        \begin{itemize}
          \item $\vect{x}^{(t+1)} \gets \vect{x}^{(t)} - \gamma\nabla
            f\left(\vect{x}^{(t)}\right)$
        \end{itemize}
    \end{itemize}
  \item simple to implement
  \item good scalability and robustness
  \item \emph{stepsize} $\gamma$ usually decreasing with
    $\gamma\approx\frac{1}{t}$
\end{itemize}

\subsubsection*{Stochastic Gradient Descent}
\begin{itemize}
  \item Optimization Problem Structure: minimize $f(\vect{x}) = \frac{1}{N}
    \sum_{n=1}^N f_n(\vect{x})$ with $\vect{x} \in \R^D$
\end{itemize}
\begin{itemize}
  \item Algorithm:
    \begin{itemize}
      \item initialize $\vect{x}^{(0)} \in \R^D$
      \item for $t = 0,\ldots,\mathrm{maxIter}$
        \begin{itemize}
          \item $n \gets \mathcal{U}\{1,N\}$
          \item $\vect{x}^{(t+1)} \gets \vect{x}^{(t)} - \gamma\nabla
            f_n\left(\vect{x}^{(t)}\right)$
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection*{Duality for Constrained Optimization}
\begin{itemize}
  \item Constrained Problem Formulation (Standard Form): minimize $f(\vect{x})$
    subject to $g_i(\vect{x}) \leq 0$, $i = 1, \ldots, m$, $h_i(\vect{x}) = 0$,
    $i = 1,\ldots,p$
  \item Unconstrained Problem: minimze $f(\vect{x}) + \sum_{i=1}^m
    I_{-}(g_i(\vect{x})) + \sum_{i=1}^p I_{0}(h_i(\vect{x}))$. $I_{-}$ and $I_{0}$
    are ``brickwall'' indicator functions.
\end{itemize}

\subsubsection*{Dual Problem}
\begin{itemize}
  \item Lagrangian: $L(\vect{x},\vect{\lambda},\vect{\nu}) := f(\vect{x}) +
    \sum_{i=1}^m \lambda_i g_i(\vect{x}) + \sum_{i=1}^p \nu_i h_i(\vect{x})$
  \item Lagrange dual function: $d(\vect{\lambda},\vect{\nu}) := \inf_{\vect{x}}
    L(\vect{x},\vect{\lambda},\vect{\nu})$
  \item Lagrange \emph{dual problem}: maximize $d(\vect{\lambda},\vect{\nu})$
    subject to $\vect{\lambda} \geq \vect{0}$.
    \begin{itemize}
      \item It is always a lower bound on the primal value $f(\vect{x})$ of any
        feasible $\vect{x}$ and thus a lower bound on the unknown solution value
        $f(\vect{x}^*)$ of the primal problem.
      \item Strong Duality: If the primal optimization problem is convex and
        under some additional conditions, the solution value of the dual
        problem is \emph{equal} to the solution value $f(\vect{x}^*)$ of the
        primal problem.
    \end{itemize}
\end{itemize}

\subsubsection*{Convexity}
\begin{itemize}
  \item Convex Set: A set $Q$ is convex if for any $\vect{x},\vect{y} \in Q$
    and any $\theta \in [0,1]$, we have $\theta\vect{x} + (1-\theta) \vect{y}
    \in Q$.
  \item Convex Function: A function $f\from\R^D\to\R$ is convex if $\dom f$ is
    a convex set and if for all $\vect{x},\vect{y}\in\dom f$, and $\theta\in
    [0,1]$ we have $f(\theta\vect{x} + (1-\theta)\vect{y}) \leq \theta
    f(\vect{x}) + (1-\theta)f(\vect{y})$.
  \item Convex Optimization: Convex Optimization Problems are of the form $\min
    f(\vect{x})$ s.t.\ $\vect{x}\in Q$ where both $f$ is a convex function and
    $Q$ is a convex set (note: $\R^D$ is convex). In Convex Optimization
    Problems every local minimum is a \emph{global minimum}.

\end{itemize}

\section*{Robust PCA}

\end{document}
